{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dccfc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "\n",
      "--- (a) Initialization ---\n",
      "Tensor from list:\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]], device='mps:0')\n",
      "Random Tensor:\n",
      "tensor([[0.4578, 0.9106],\n",
      "        [0.5090, 0.1133]], device='mps:0')\n",
      "\n",
      "--- (b) Operations ---\n",
      "Addition:\n",
      "tensor([[1.4578, 2.9106],\n",
      "        [3.5090, 4.1133]], device='mps:0')\n",
      "Matrix Multiplication:\n",
      "tensor([[1.4758, 1.1372],\n",
      "        [3.4095, 3.1851]], device='mps:0')\n",
      "Reshaped (4,): tensor([1., 2., 3., 4.], device='mps:0')\n",
      "\n",
      "--- (c) Autograd ---\n",
      "x: 2.0\n",
      "y: 13.0\n",
      "Gradient (dy/dx at x=2): 12.0\n"
     ]
    }
   ],
   "source": [
    "#Question 1: Introduction of PyTorch Tensors and Basic Operations\n",
    "\n",
    "import torch\n",
    "\n",
    "# Check for Mac M4 GPU support\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Part (a) Initialization & Data Types ---\n",
    "print(\"\\n--- (a) Initialization ---\")\n",
    "# Create a tensor from a list (2D tensor)\n",
    "t_data = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32, device=device)\n",
    "# Create with random values (random weights initialization)\n",
    "t_rand = torch.rand((2, 2), device=device)\n",
    "# Create zeros (for bias initialization)\n",
    "t_zeros = torch.zeros((2, 2), device=device)\n",
    "\n",
    "print(f\"Tensor from list:\\n{t_data}\")\n",
    "print(f\"Random Tensor:\\n{t_rand}\")\n",
    "\n",
    "# --- Part (b) Operations ---\n",
    "print(\"\\n--- (b) Operations ---\")\n",
    "# Arithmetic\n",
    "t_add = t_data + t_rand # Element-wise addition (for combining weights and biases, wx+b)\n",
    "t_mul = t_data * t_rand  # Element-wise multiplication (for feature scaling)\n",
    "t_matmul = torch.matmul(t_data, t_rand) # Matrix multiplication (for feature extraction)\n",
    "\n",
    "print(f\"Addition:\\n{t_add}\")\n",
    "print(f\"Matrix Multiplication:\\n{t_matmul}\")\n",
    "\n",
    "#broadcasting- adding a smaller tensor to a larger one automatically\n",
    "#t_broadcast = t_data + torch.tensor([1, 0], device=device) # Broadcasting addition\n",
    "#print(f\"Broadcasting Addition:\\n{t_broadcast}\")\n",
    "\n",
    "#indexing- accessing specific elements\n",
    "#t_index = t_data[1, :] # Second row\n",
    "#print(f\"Indexing (second row): {t_index}\")\n",
    "\n",
    "# Reshaping\n",
    "t_flat = t_data.view(4) # Flatten to 1D\n",
    "print(f\"Reshaped (4,): {t_flat}\")\n",
    "\n",
    "#Autograd (Automatic Differentiation): Neural networks learn by minimizing error.\n",
    "# To do this, they need to know the \"slope\" (gradient) of the error with respect to each weight.\n",
    "# PyTorch builds a \"computational graph\" as you perform operations, \n",
    "# allowing it to calculate these slopes automatically using the chain rule.\n",
    "# --- Part (c) Autograd ---\n",
    "print(\"\\n--- (c) Autograd ---\")\n",
    "# Create a tensor that requires gradient calculation\n",
    "x = torch.tensor(2.0, requires_grad=True, device=device)\n",
    "y = x ** 3 + 5  # Function: y = x^3 + 5\n",
    "\n",
    "# Backward pass to calculate gradient (dy/dx = 3x^2)\n",
    "y.backward()\n",
    "\n",
    "print(f\"x: {x.item()}\")\n",
    "print(f\"y: {y.item()}\")\n",
    "print(f\"Gradient (dy/dx at x=2): {x.grad.item()}\") # Should be 3*(2)^2 = 12, Stores the result of that derivative calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b6dd4f4-a542-47d7-8205-594976ff552e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T08:43:49.443299Z",
     "iopub.status.busy": "2026-01-16T08:43:49.443052Z",
     "iopub.status.idle": "2026-01-16T08:43:55.056765Z",
     "shell.execute_reply": "2026-01-16T08:43:55.055522Z",
     "shell.execute_reply.started": "2026-01-16T08:43:49.443273Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (26.0rc2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.1)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (14.2.0)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2026.1.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c79a94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T08:44:20.343765Z",
     "iopub.status.busy": "2026-01-16T08:44:20.343000Z",
     "iopub.status.idle": "2026-01-16T08:44:39.697115Z",
     "shell.execute_reply": "2026-01-16T08:44:39.696212Z",
     "shell.execute_reply.started": "2026-01-16T08:44:20.343718Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-16 08:44:22.512693: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768553062.769346      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1768553062.845198      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1768553063.467525      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768553063.467581      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768553063.467584      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768553063.467586      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TensorFlow Linear Algebra ---\n",
      "Matrix A:\n",
      "[[1. 2.]\n",
      " [3. 4.]]\n",
      "Matrix B:\n",
      "[[5. 6.]\n",
      " [7. 8.]]\n",
      "\n",
      "Matrix Multiplication (A x B):\n",
      "[[19. 22.]\n",
      " [43. 50.]]\n",
      "\n",
      "Transpose of A:\n",
      "[[1. 3.]\n",
      " [2. 4.]]\n",
      "\n",
      "Determinant of A: -2.0\n",
      "\n",
      "Inverse of A:\n",
      "[[-2.0000002   1.0000001 ]\n",
      " [ 1.5000001  -0.50000006]]\n",
      "\n",
      "Element-wise Addition:\n",
      "[[ 6.  8.]\n",
      " [10. 12.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-16 08:44:39.591059: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "#Question 2: Perform all Linear Algebra operations with TensorFlow\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Suppress TensorFlow logs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "print(\"--- TensorFlow Linear Algebra ---\")\n",
    "\n",
    "# Define two matrices\n",
    "A = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n",
    "B = tf.constant([[5, 6], [7, 8]], dtype=tf.float32)\n",
    "\n",
    "print(f\"Matrix A:\\n{A.numpy()}\")\n",
    "print(f\"Matrix B:\\n{B.numpy()}\")\n",
    "\n",
    "# 1. Matrix Multiplication\n",
    "matmul = tf.matmul(A, B)\n",
    "print(f\"\\nMatrix Multiplication (A x B):\\n{matmul.numpy()}\")\n",
    "\n",
    "# 2. Transpose\n",
    "transpose = tf.transpose(A)\n",
    "print(f\"\\nTranspose of A:\\n{transpose.numpy()}\")\n",
    "\n",
    "# 3. Determinant\n",
    "det = tf.linalg.det(A)\n",
    "print(f\"\\nDeterminant of A: {det.numpy()}\")\n",
    "\n",
    "# 4. Inverse -> (1/|A|)(adj A)\n",
    "inv = tf.linalg.inv(A)\n",
    "print(f\"\\nInverse of A:\\n{inv.numpy()}\")\n",
    "\n",
    "# 5. Element-wise operations\n",
    "add = tf.add(A, B)\n",
    "print(f\"\\nElement-wise Addition:\\n{add.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25670faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- AND Gate Predictions ---\n",
      "[0 0] -> 0\n",
      "[0 1] -> 0\n",
      "[1 0] -> 0\n",
      "[1 1] -> 1\n",
      "\n",
      "--- OR Gate Predictions ---\n",
      "[0 0] -> 0\n",
      "[0 1] -> 1\n",
      "[1 0] -> 1\n",
      "[1 1] -> 1\n"
     ]
    }
   ],
   "source": [
    "#Question 3: Implement AND & OR Gates using Perceptron\n",
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, input_size, learning_rate=0.1, epochs=100):\n",
    "        self.weights = np.zeros(input_size + 1) # +1 for bias\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        #Why learning_rate=0.1? It’s a \"Goldilocks\" value—small enough to be stable,\n",
    "        #but large enough to learn the AND/OR gates in just a few dozen rounds.\n",
    "\n",
    "    def activation(self, x): #step function as its binary classification (0 or 1 output)\n",
    "        return 1 if x >= 0 else 0\n",
    "\n",
    "    def predict(self, x): #x is input vector (input features)\n",
    "        # z = w1*x1 + w2*x2 + ... + bias\n",
    "        z = np.dot(x, self.weights[1:]) + self.weights[0]\n",
    "        return self.activation(z)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        for _ in range(self.epochs):\n",
    "            for inputs, label in zip(X, y):\n",
    "                prediction = self.predict(inputs)\n",
    "                # Weight update rule: w = w + lr * (target - pred) * input\n",
    "                self.weights[1:] += self.lr * (label - prediction) * inputs\n",
    "                # Update bias: bias = bias + lr * (target - pred)\n",
    "                self.weights[0] += self.lr * (label - prediction)\n",
    "\n",
    "# Data for AND Gate\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_and = np.array([0, 0, 0, 1])\n",
    "\n",
    "# Data for OR Gate\n",
    "y_or = np.array([0, 1, 1, 1])\n",
    "\n",
    "# --- Train AND Gate ---\n",
    "p_and = Perceptron(input_size=2)\n",
    "p_and.train(X, y_and)\n",
    "print(\"\\n--- AND Gate Predictions ---\")\n",
    "for x in X:\n",
    "    print(f\"{x} -> {p_and.predict(x)}\")\n",
    "\n",
    "# --- Train OR Gate ---\n",
    "p_or = Perceptron(input_size=2)\n",
    "p_or.train(X, y_or)\n",
    "print(\"\\n--- OR Gate Predictions ---\")\n",
    "for x in X:\n",
    "    print(f\"{x} -> {p_or.predict(x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41db3eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- XOR Problem Predictions ---\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "#Question 4: Implementation of XOR Problem using PyTorch Neural Network\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# XOR Data (Inputs and Targets)\n",
    "X = torch.tensor([[0,0], [0,1], [1,0], [1,1]], dtype=torch.float32).to(device)\n",
    "y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32).to(device)\n",
    "\n",
    "# Define Neural Network\n",
    "class XORNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XORNet, self).__init__()\n",
    "        self.hidden = nn.Linear(2, 4) # Input: 2 -> Hidden: 4\n",
    "        self.output = nn.Linear(4, 1) # Hidden: 4 -> Output: 1\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden(x)) # negative values become zero,\n",
    "        #introducing non-linearity(not a straight line)\n",
    "        x = self.sigmoid(self.output(x)) # Sigmoid to get output between 0 and 1 (1/1+exp(-x))\n",
    "        return x\n",
    "    #forward pass - computing output from input through layers\n",
    "\n",
    "model = XORNet().to(device)\n",
    "criterion = nn.BCELoss() # Binary Cross Entropy Loss for binary classification\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1) # Stochastic Gradient Descent for updating weights\n",
    "\n",
    "# Training\n",
    "for epoch in range(5000):\n",
    "    optimizer.zero_grad() # Clear old gradients\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    loss.backward() # Backpropagation (calculating new gradients)\n",
    "    #backpropagation- process of updating weights based on the error (loss) calculated.\n",
    "    optimizer.step() # Update weights\n",
    "\n",
    "# Testing\n",
    "print(\"\\n--- XOR Problem Predictions ---\")\n",
    "with torch.no_grad():\n",
    "    preds = model(X)\n",
    "    print(preds.cpu().round()) # Move to CPU to print, round to 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11576417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/1000], Loss: 0.5073\n",
      "Epoch [400/1000], Loss: 0.5010\n",
      "Epoch [600/1000], Loss: 0.4981\n",
      "Epoch [800/1000], Loss: 0.4962\n",
      "Epoch [1000/1000], Loss: 0.4946\n",
      "\n",
      "--- Regression Prediction ---\n",
      "Predicted Price for 3 Bed / 2000 Sqft: $496,653.75\n"
     ]
    }
   ],
   "source": [
    "#Question 5: Implement Simple Neural Network to solve Regression Problem\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# 1. Load Data\n",
    "filename = 'house_price_full+(2) - house_price_full+(2).csv'\n",
    "data = pd.read_csv(filename)\n",
    "X = data[['bedrooms', 'sqft_living']].values\n",
    "y = data['price'].values.reshape(-1, 1)\n",
    "\n",
    "# 2. Preprocessing\n",
    "scaler_x = StandardScaler() #shifts the data so the mean is 0 and the standard deviation is 1.\n",
    "scaler_y = StandardScaler()\n",
    "X_scaled = scaler_x.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(device) #input features\n",
    "y_tensor = torch.tensor(y_scaled, dtype=torch.float32).to(device) #target prices\n",
    "\n",
    "# 3. Define Simple Regression Model (Matches the diagram structure concept)\n",
    "class RegressionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegressionNet, self).__init__()\n",
    "        # Assuming the diagram in PDF shows: Inputs -> Hidden -> Output\n",
    "        self.layer1 = nn.Linear(2, 4)  # 2 Inputs (bedrooms, sqft), 4 Hidden layers\n",
    "        self.layer2 = nn.Linear(4, 1)  # 4 hidden layers, 1 Output (price)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x)) #negative values become zero\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "#Activation functions are used in neural networks to introduce non-linearity,\n",
    "# allowing the network to learn complex patterns and relationships in data,\n",
    "#otherwise, the network would just be a linear regression model, unable to solve intricate problems.\n",
    "\n",
    "model = RegressionNet().to(device)\n",
    "criterion = nn.MSELoss() # Mean Squared Error Loss for regression, y=(predicted - actual)^2\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01) # Stochastic Gradient Descent for updating weights\n",
    "\n",
    "# 4. Training\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    outputs = model(X_tensor)\n",
    "    loss = criterion(outputs, y_tensor)\n",
    "    \n",
    "    optimizer.zero_grad() # Clear old gradients\n",
    "    loss.backward() # Backpropagation (calculating new gradients)\n",
    "    #backpropagation- process of updating weights based on the error (loss) calculated.\n",
    "    optimizer.step() # Update weights\n",
    "    \n",
    "    if (epoch+1) % 200 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 5. Prediction Test\n",
    "print(\"\\n--- Regression Prediction ---\")\n",
    "# Test: 3 bedrooms, 2000 sqft (forward pass through the network to get predicted price)\n",
    "sample = scaler_x.transform([[3, 2000]])\n",
    "sample_tensor = torch.tensor(sample, dtype=torch.float32).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction_scaled = model(sample_tensor)\n",
    "    prediction = scaler_y.inverse_transform(prediction_scaled.cpu().numpy()) #Because the model predicts\n",
    "    #a \"scaled\" number (like 0.45), we have to reverse the scaling to get \n",
    "    # the actual dollar amount ($496,653.75).\n",
    "\n",
    "print(f\"Predicted Price for 3 Bed / 2000 Sqft: ${prediction[0][0]:,.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
