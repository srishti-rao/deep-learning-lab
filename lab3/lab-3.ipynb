{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727ed075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import vgg16, resnet50, resnet101, efficientnet_b0, inception_v3, mobilenet_v2\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# --- 1. CONFIGURATION & DEVICE SETUP (Mac M4 Optimization) ---\n",
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "BATCH_SIZE = 64 # Reduce if you run out of memory\n",
    "IMG_SIZE = 224  # Standard size for ImageNet models (VGG, ResNet, etc.)\n",
    "EPOCHS = 5      # Kept low for demonstration; increase to 10-20 for real results\n",
    "\n",
    "# --- 2. DATA PREPARATION ---\n",
    "def get_dataloaders(dataset_name='CIFAR10'):\n",
    "    print(f\"Preparing {dataset_name} data...\")\n",
    "    \n",
    "    # Models like VGG/ResNet expect 224x224. We resize CIFAR/MNIST.\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.Grayscale(num_output_channels=3) if dataset_name == 'MNIST' else transforms.Lambda(lambda x: x),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    if dataset_name == 'CIFAR10':\n",
    "        trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "        testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "        num_classes = 10\n",
    "    elif dataset_name == 'MNIST':\n",
    "        trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "        testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "        num_classes = 10\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    return trainloader, testloader, num_classes\n",
    "\n",
    "# --- 3. MODEL ARCHITECTURES (Part 1) ---\n",
    "\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.feature_extractor = nn.Sequential(            \n",
    "            nn.Conv2d(3, 6, kernel_size=5),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "            nn.Conv2d(6, 16, kernel_size=5),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "            nn.Conv2d(16, 120, kernel_size=5),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(120, 84),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(84, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Handle dynamic input size logic if needed, but for 224x224 input:\n",
    "        # Note: LeNet is tiny, running it on 224x224 is overkill, but kept for compatibility\n",
    "        x = F.adaptive_avg_pool2d(self.feature_extractor(x), (1, 1)) \n",
    "        x = torch.flatten(x, 1)\n",
    "        logits = self.classifier(x)\n",
    "        return logits, x # Return features for t-SNE\n",
    "\n",
    "# Wrapper to make torchvision models compatible with our loop\n",
    "class ModelWrapper(nn.Module):\n",
    "    def __init__(self, model_name, num_classes):\n",
    "        super(ModelWrapper, self).__init__()\n",
    "        if model_name == 'alexnet':\n",
    "            self.model = torchvision.models.alexnet(weights=None)\n",
    "            self.model.classifier[6] = nn.Linear(4096, num_classes)\n",
    "        elif model_name == 'vgg':\n",
    "            self.model = vgg16(weights=None)\n",
    "            self.model.classifier[6] = nn.Linear(4096, num_classes)\n",
    "        elif model_name == 'resnet50':\n",
    "            self.model = resnet50(weights=None)\n",
    "            self.model.fc = nn.Linear(2048, num_classes)\n",
    "        elif model_name == 'resnet101':\n",
    "            self.model = resnet101(weights=None)\n",
    "            self.model.fc = nn.Linear(2048, num_classes)\n",
    "        elif model_name == 'efficientnet':\n",
    "            self.model = efficientnet_b0(weights=None)\n",
    "            self.model.classifier[1] = nn.Linear(1280, num_classes)\n",
    "        elif model_name == 'mobilenet':\n",
    "            self.model = mobilenet_v2(weights=None)\n",
    "            self.model.classifier[1] = nn.Linear(1280, num_classes)\n",
    "        elif model_name == 'inception':\n",
    "             # Inception needs 299x299 usually, we force it to handle our flow or rely on adaptive pool\n",
    "            self.model = inception_v3(weights=None, aux_logits=True) \n",
    "            self.model.fc = nn.Linear(2048, num_classes)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        if hasattr(self.model, 'aux_logits') and self.model.aux_logits:\n",
    "            if self.training:\n",
    "                x, _ = self.model(x) # Ignore aux output during train loop\n",
    "            else:\n",
    "                x = self.model(x)\n",
    "        else:\n",
    "            x = self.model(x)\n",
    "        return x, x # Returning output twice to match LeNet signature (logits, features)\n",
    "\n",
    "# --- 4. ADVANCED LOSS FUNCTIONS (Part 2) ---\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "# ArcFace requires a specific Layer, not just a loss function\n",
    "class ArcFaceLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.50):\n",
    "        super(ArcFaceLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, input, label=None):\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        if label is None:\n",
    "            return cosine * self.s\n",
    "        \n",
    "        # Add margin\n",
    "        theta = torch.acos(torch.clamp(cosine, -1.0 + 1e-7, 1.0 - 1e-7))\n",
    "        target_logits = torch.cos(theta + self.m)\n",
    "        \n",
    "        one_hot = torch.zeros_like(cosine)\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        \n",
    "        output = (one_hot * target_logits) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "        return output\n",
    "\n",
    "# --- 5. TRAINING & EVALUATION ENGINE ---\n",
    "\n",
    "def train_eval(model, train_loader, test_loader, criterion, optimizer, epochs, model_name=\"Model\"):\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "    \n",
    "    # Check if model has ArcFace layer (special handling)\n",
    "    is_arcface = isinstance(model, ArcFaceLayer) or hasattr(model, 'arcface')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs, features = model(inputs)\n",
    "            \n",
    "            # Loss calculation\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "        train_acc = 100 * correct / total\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        all_features = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs, features = model(inputs)\n",
    "                _, predicted = outputs.max(1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "                # Collect for t-SNE (only on last epoch)\n",
    "                if epoch == epochs - 1:\n",
    "                    all_features.append(features.cpu())\n",
    "                    all_labels.append(labels.cpu())\n",
    "\n",
    "        test_acc = 100 * test_correct / test_total\n",
    "        test_accs.append(test_acc)\n",
    "        \n",
    "        print(f\"[{model_name}] Epoch {epoch+1}/{epochs} | Loss: {running_loss/len(train_loader):.4f} | Train Acc: {train_acc:.2f}% | Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "    return train_accs, test_accs, torch.cat(all_features), torch.cat(all_labels)\n",
    "\n",
    "# --- 6. VISUALIZATION (Part 3) ---\n",
    "\n",
    "def plot_tsne(features, labels, title=\"t-SNE\"):\n",
    "    print(f\"Generating t-SNE plot for {title}...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    # Take a subset if data is huge to speed up\n",
    "    subset_idx = np.random.choice(features.shape[0], min(1000, features.shape[0]), replace=False)\n",
    "    features_subset = features[subset_idx]\n",
    "    labels_subset = labels[subset_idx]\n",
    "    \n",
    "    tsne_results = tsne.fit_transform(features_subset)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(\n",
    "        x=tsne_results[:,0], y=tsne_results[:,1],\n",
    "        hue=labels_subset, palette=sns.color_palette(\"hls\", 10),\n",
    "        legend=\"full\", alpha=0.7\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# --- 7. MAIN EXECUTION ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Part 1: Architecture Comparison (Example: VGG vs LeNet on CIFAR10) ---\n",
    "    print(\"\\n=== PART 1: Architecture Comparison ===\")\n",
    "    train_loader, test_loader, num_classes = get_dataloaders('CIFAR10')\n",
    "    \n",
    "    # 1. LeNet5\n",
    "    lenet = LeNet5(num_classes).to(DEVICE)\n",
    "    opt = optim.Adam(lenet.parameters(), lr=0.001)\n",
    "    train_eval(lenet, train_loader, test_loader, nn.CrossEntropyLoss(), opt, EPOCHS, \"LeNet5\")\n",
    "\n",
    "    # 2. VGG (Example of large model)\n",
    "    # vgg = ModelWrapper('vgg', num_classes).to(DEVICE)\n",
    "    # opt = optim.SGD(vgg.parameters(), lr=0.01, momentum=0.9)\n",
    "    # train_eval(vgg, train_loader, test_loader, nn.CrossEntropyLoss(), opt, EPOCHS, \"VGG16\")\n",
    "\n",
    "    # --- Part 2 & 3: Loss Function Study (Example: ResNet with Focal vs BCE) ---\n",
    "    print(\"\\n=== PART 2 & 3: Loss Functions & Visualization ===\")\n",
    "    \n",
    "    # Model for experiments\n",
    "    base_model = ModelWrapper('resnet50', num_classes).to(DEVICE)\n",
    "    \n",
    "    # Experiment A: Binary Cross Entropy (Simulated via CrossEntropy for Multiclass)\n",
    "    print(\"--- Training with Cross Entropy (Standard) ---\")\n",
    "    optimizer = optim.Adam(base_model.parameters(), lr=0.001)\n",
    "    _, _, feats_ce, lbls_ce = train_eval(base_model, train_loader, test_loader, nn.CrossEntropyLoss(), optimizer, 3, \"ResNet_CE\") # Low epochs for speed\n",
    "    plot_tsne(feats_ce, lbls_ce, \"t-SNE: Cross Entropy Loss\")\n",
    "\n",
    "    # Experiment B: Focal Loss\n",
    "    print(\"--- Training with Focal Loss ---\")\n",
    "    # Re-init model\n",
    "    base_model = ModelWrapper('resnet50', num_classes).to(DEVICE) \n",
    "    optimizer = optim.Adam(base_model.parameters(), lr=0.001)\n",
    "    _, _, feats_foc, lbls_foc = train_eval(base_model, train_loader, test_loader, FocalLoss(gamma=2), optimizer, 3, \"ResNet_Focal\")\n",
    "    plot_tsne(feats_foc, lbls_foc, \"t-SNE: Focal Loss\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
